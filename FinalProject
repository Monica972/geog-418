##Data prep
install.packages("sf")
install.packages("plyr")
install.packages("dplyr")
install.packages("spdep")
install.packages("GISTools")
install.packages("raster")
install.packages("maptools")
install.packages("rgdal")
install.packages("spatstat")
install.packages("sp")
install.packages("tmap")
install.packages ("tmaptools")
install.packages("gstat")
install.packages("grid")
install.packages("gtable")
install.packages("gridExtra")
install.packages("spgwr")
install.packages("PBSmapping")
install.packages("rgeos")

#Libraries
library(sf)
library(plyr)
library(dplyr)
library(spdep)
library(GISTools)
library(raster)
library(maptools)
library(rgdal)
library(spatstat)
library(sp)
library(spatstat)
library(tmap)
library(tmaptools)
library(gstat)
library(grid)
library(gtable)
library(gridExtra)
library(spgwr)
library(PBSmapping)
library(rgeos)


#Set working directory
setwd("Z:/Final Project")

#Reading in particulate matter dataset
pm25 <- read.csv("./Originals/geog418-518-2019-finalproject-master/PM25.csv") #Read in PM2.5 data
#Select only columns 1 and 2
pm25 <- pm25[,1:2]
#Change the column names 
colnames(pm25) <- c("POSTALCODE", "PM25")
pm25 <- na.omit(pm25)

#Reading in postal code shapefile
postalcodes <- readOGR("./Originals/geog418-518-2019-finalproject-master/BC_PostalCodes", "BC_Postal_Codes") #Read in related postal code data

#Reading in dissemination tract and income data
income <- read.csv("./Originals/geog418-518-2019-finalproject-master/Income.csv") #Read in census income data  
colnames(income) <- c("DAUID", "Income") #Select only ID and Income columns
census.tracts <- readOGR("./Originals/geog418-518-2019-finalproject-master/BC_DA","BC_DA") #Read in dissemination tract shapefile
income.tracts <- merge(census.tracts,income, by = "DAUID") #Merge income and dissemination data
nrow(income.tracts) #Determine the number of columns in the dataframe
income.tracts <- income.tracts[!is.na(income.tracts$Income),]

#Create choropleth map of income
med.income <- income.tracts$Income
shades <- auto.shading(med.income, n=6, cols = brewer.pal(6, 'Oranges'))
choropleth(income.tracts, med.income, shades, main= "Median Income in Greater Vancouver, BC") #map the data with associated colours
choro.legend(-123.4935, 49.13, shades, cex = 0.6, title = "Median Income") #add a legend (you might need to change the location)

#Select postal codes that fall within dissemination tracts)
postalcodes <- intersect(postalcodes,income.tracts)
plot(postalcodes) #See what the data looks like spatially
head(postalcodes) #See what the data looks like in tabular form

#Join PM2.5 data with postal code data
pm25.spatial <- merge(postalcodes,pm25,by = "POSTALCODE")

#Aggregate the PM2.5 values in each DA in order to have a single value per DA. Here we aggregate based on the mean.
pm25.aggregate <- aggregate((as.numeric(pm25.spatial$PM25)/10)~pm25.spatial$DAUID,FUN=max)

#Re-join aggregated data to the income.tracts layer.
colnames(pm25.aggregate) <- c("DAUID", "PM25AGG") #Select only ID and Income columns
income.pm25 <- merge(income.tracts,pm25.aggregate, by = "DAUID") #Merge income and dissemination data
income.pm25 <- income.pm25[!is.na(income.pm25$PM25AGG),]

#Re-join aggregated data to the pm25.spatial points layer.
pm25.points.aggregate <- merge(pm25.spatial, pm25.aggregate, by = "DAUID")

#Create a subsample of the datapoints provided in the PM2.5 dataset using the sample n provided on CourseSpaces
## THIS IS WHERE TO CHANGE SAMPLE SIZE
sampleSize=300
spSample <- pm25.points.aggregate[sample(1:length(pm25.points.aggregate),sampleSize),]
plot(spSample)

#Create a grid called grd to use in your interpolation
# Create an empty grid where n is the total number of cells
grd <- as.data.frame(spsample(spSample, "regular", n=5000))
names(grd)       <- c("X", "Y")
coordinates(grd) <- c("X", "Y")
gridded(grd)     <- TRUE  # Create SpatialPixel object
fullgrid(grd)    <- TRUE  # Create SpatialGrid object
proj4string(grd) <- proj4string(spSample)


##GLOBAL AND LOCAL MORANS I on median income
#Determine neighbour polygons

inc.nb <- poly2nb(income.tracts)
inc.net <- nb2lines(inc.nb,coords=coordinates(income.tracts)) #convert neighbour matrix into lines to plot - show for every polygon which are considered neighbours this is queens case 


tm_shape(income.tracts) + tm_borders(col='lightgrey') + 
  tm_shape(inc.net) + tm_lines(col='red') +
  tm_add_legend(title = 'Legend', type= 'line', labels='Polygon Neighbours', col= 'red', legend.format = list("BOTTOM", "LEFT")) +
  tm_layout(title= "Polygon Neighbours with Queens Case")


inc.nb2 <- poly2nb(income.tracts, queen = FALSE) # queen false therefore uses rooks case for second plot
inc.net2 <- nb2lines(inc.nb2,coords=coordinates(income.tracts))

tm_shape(income.tracts) + tm_borders(col='lightgrey') + 
  tm_shape(inc.net) + tm_lines(col='blue', lwd = 2) +
  tm_shape(inc.net2) + tm_lines(col='orange', lwd = 2) +
  tm_add_legend(title = 'Legend',type = 'line',labels = "Queens Case", col= 'blue', lwd=2) +
  tm_add_legend(type = 'line', labels = "Rooks Case", col='orange', lwd=2, legend.format = list("BOTTOM", "LEFT")) +
  tm_layout(title= "Polygon Neighbours with Queens Case and Rooks Case")

######################## 

inc.lw <- nb2listw(inc.nb, zero.policy = TRUE, style = "W")
print.listw(inc.lw, zero.policy = TRUE) #weight matrix

###Global morans I

mi <- moran.test(med.income, inc.lw, zero.policy = TRUE)
mi


moran.range <- function(lw) {
  wmat <- listw2mat(lw)
  return(range(eigen((wmat + t(wmat))/2)$values))
}

moran.range(inc.lw)

mI <- mi$estimate[[1]]
eI <- mi$estimate[[2]]
var <- mi$estimate[[3]]
z <-(mI-eI)/(sqrt(var))

mI <- round(mI, digits = 5)
eI <- round(eI, digits = 5)
var<- round(var, digits = 5)
z <- round(z, digits = 5)

######### make table

data.for.table1 = data.frame(mI, eI, var, z)

table1 <- tableGrob(data.for.table1, cols=c("Moran's I", "Expected Moran's I", "Variance", "Z-Value"), rows = " ") #make a table "Graphical Object" (GrOb) 
t1Caption <- textGrob("Table 1: Global Moran's I for Median Income in Vancouver", gp = gpar(fontsize = 09))
padding <- unit(5, "mm")

table1 <- gtable_add_rows(table1, 
                          heights = grobHeight(t1Caption) + padding, 
                          pos = 0)

table1 <- gtable_add_grob(table1,
                          t1Caption, t = 1, l = 2, r = ncol(data.for.table1) + 1)


grid.arrange(table1, newpage = TRUE)

##Local morans I

lisa.test <- localmoran(med.income, inc.lw)
income.tracts$Ii <- lisa.test[,1]
income.tracts$E.Ii<- lisa.test[,2]
income.tracts$Var.Ii<- lisa.test[,3]
income.tracts$Z.Ii<- lisa.test[,4]
income.tracts$P<- lisa.test[,5]

#map local morans I
map_LISA <- tm_shape(income.tracts) + 
  tm_polygons(col = "Ii", 
              title = "Local Moran's I", 
              style = "fisher", 
              palette = "viridis", n = 6) 
map_LISA

## map p-values
map_pval <- tm_shape(income.tracts) + 
  tm_polygons(col = "P", 
              title = "P-Values for Local Moran's I", 
              style = "fixed", breaks = c(0, 0.005, 0.025, 0.05, 1.0), 
              palette = "Blues", n = 4, contrast = c(0.5, 1))  +
  tm_layout(title= "P-Values for the Local Morans I of Median Income in Greater Vancouver, BC", legend.position = c("LEFT", "BOTTOM")) 

map_pval

##Morans I plot

moran.plot(med.income, inc.lw, zero.policy=NULL, spChk=NULL, labels=NULL, xlab="Median Income", 
           ylab="Spatially Lagged Median Income", quiet=NULL)


##SPATIAL INTERPOLATION
#IDW

P.idw <- gstat::idw(PM25AGG ~ 1, spSample, newdata=grd, idp=5.5) #can play around with p value

r       <- raster(P.idw)

tm_shape(r) +   
  tm_raster(n=10,palette = "YlOrRd", title="PM2.5 \n(in ppm)") + 
  tm_shape(spSample) + tm_dots(size=0.2) +
  tm_legend(legend.outside=FALSE, legend.position = c("left", "bottom")) +
  tm_layout(title= "IDW Interpolation of PM 2.5 in Greater Vancouver", inner.margins = 0.01) +
  tm_compass(type="8star", position = c("right", "bottom"))


# Leave-one-out validation routine

IDW.out <- vector(length = length(spSample))

for (i in 1:length(spSample)) { 
  IDW.out[i] <- gstat::idw(PM25AGG ~ 1, spSample[-i,], spSample[i,], idp=5.5)$var1.pred  
}

# Plot the differences

OP <- par(pty="s", mar=c(4,3,0,0))

plot(IDW.out ~ spSample$PM25AGG, asp=1, xlab="Observed", ylab="Predicted", pch=16,   
     col=rgb(0,0,0,0.5))

abline(lm(IDW.out ~ spSample$PM25AGG), col="red", lw=2,lty=2)
abline(0,1)
par(OP)
sqrt( sum((IDW.out - spSample$PM25AGG)^2) / length(spSample))

#rmse ## change p val to keep getting lower error values


##Implementation of a jackknife technique to estimate a confidence interval at each unsampled point.

# Create the interpolated surface

img <- gstat::idw(PM25AGG~1, spSample, newdata=grd, idp=5.5)
n   <- length(spSample)
Zi  <- matrix(nrow = length(img$var1.pred), ncol = n)


# Remove a point then interpolate (do this n times for each point)

st <- stack()

for (i in 1:n){  
  Z1 <- gstat::idw(PM25AGG~1, spSample[-i,], newdata=grd, idp=5.5) 
  st <- addLayer(st,raster(Z1,layer=1)) # Calculated pseudo-value Z at j
  Zi[,i] <- n * img$var1.pred - (n-1) * Z1$var1.pred 
}


# Jackknife estimator of parameter Z at location j

Zj <- as.matrix(apply(Zi, 1, sum, na.rm=T) / n )


# Compute (Zi* - Zj)^2

c1 <- apply(Zi,2,'-',Zj)            # Compute the difference
c1 <- apply(c1^2, 1, sum, na.rm=T ) # Sum the square of the difference


# Compute the confidence interval

CI <- sqrt( 1/(n*(n-1)) * c1)


# Create (CI / interpolated value) raster

img.sig   <- img
img.sig$v <- CI /img$var1.pred 


# Clip the confidence raster to Greater Vancouver

r <- raster(img.sig, layer="v")
r.m <- mask(r, census.tracts)


# Plot the map

tm_shape(r.m) + tm_raster(n=7,title="95% confidence interval \n(in ppm)") + 
  tm_shape(spSample) + tm_dots(size=0.2) +
  tm_legend(legend.position = c("left", "bottom")) +
  tm_layout(title = "95% Confidence Interval for IDW Interpolation", inner.margins = 0.01) +
  tm_compass(type = "8star", position = c("right", "bottom"))

tmap_mode("plot")
tmap_mode("view")

######ordinary kriging
f.0 <- as.formula(PM25AGG ~ 1) 


var.smpl <- variogram(f.0, spSample, cloud = FALSE) #, cutoff=1000000, width=89900)
dat.fit  <- fit.variogram(var.smpl, fit.ranges = FALSE, fit.sills = FALSE,
                          vgm(model="Exp", psill=4.7, range = 23))
plot(var.smpl, dat.fit)

# Define the trend model
f.0 <- as.formula(PM25AGG ~ 1) 

# Perform the krige interpolation (note the use of the variogram model
# created in the earlier step)
dat.krg <- krige( f.0, spSample, grd, dat.fit)

# Convert kriged surface to a raster object for clipping
r <- raster(dat.krg)

# Plot the map
tm_shape(r) + 
  tm_raster(n=10, palette="YlOrRd",  
            title="Predicted Ozone \n(in ppm)") +
  tm_shape(spSample) + tm_dots(size=0.2) +
  tm_legend(legend.position= c("left", "bottom")) +
  tm_layout(title = "Ordinary Kriging Interpolation", inner.margins = 0.01) +
  tm_compass(type = "8star", position = c("right", "bottom"))

r   <- raster(dat.krg, layer="var1.var")

tm_shape(r) + 
  tm_raster(n=7, palette ="YlOrRd",
            title="Variance map \n(in squared ppm)") +
  tm_shape(spSample) + tm_dots(size=0.2) +
  tm_legend(legend.position = c("left", "bottom")) +
  tm_layout(title = "Variance Surface from Ordinary Kriging", inner.margins = 0.01) +
  tm_compass(type = "8star", position = c("right", "bottom"))

r   <- sqrt(raster(dat.krg, layer="var1.var")) * 1.96


tm_shape(r) + 
  tm_raster(n=7, palette ="YlOrRd",
            title="95% CI map \n(in ppm)") +
  tm_shape(spSample) + tm_dots(size=0.2) +
  tm_legend(legend.position= c("left", "bottom")) +
  tm_layout("95% Confidence Intervals for Ordinary Kriging Surface", inner.margins = 0.01) +
  tm_compass(type= "8star", position = c("right", "bottom"))


##Spatial Interpolation with Polynomial Trends
# Define the 1st order polynomial equation

f.1 <- as.formula(PM25AGG~ X + Y) 

# Add X and Y to P
spSample$X <- coordinates(spSample)[,1]
spSample$Y <- coordinates(spSample)[,2]

# Run the regression model
lm.1 <- lm( f.1, data=spSample)

# Use the regression model output to interpolate the surface
dat.1st <- SpatialGridDataFrame(grd, data.frame(var1.pred = predict(lm.1, newdata=grd))) 

# Clip the interpolated raster to Southern California
r   <- raster(dat.1st)

# Plot the map
tm_shape(r) + 
  tm_raster(n=10, palette="YlOrRd", 
            title="Predicted Ozone \n(in ppm)") +
  tm_shape(spSample) + tm_dots(size=0.2) +
  tm_legend(legend.position= c("left", "bottom")) +
  tm_layout("First Order Polynomial Surface", inner.margins = 0.01) +
  tm_compass(type= "8star", position = c("right", "bottom"))


# Define the 2nd order polynomial equation
f.2 <- as.formula(PM25AGG ~ X + Y + I(X*X)+I(Y*Y) + I(X*Y))

# Add X and Y to P
spSample$X <- coordinates(spSample)[,1]
spSample$Y <- coordinates(spSample)[,2]

# Run the regression model
lm.2 <- lm( f.2, data=spSample)

# Use the regression model output to interpolate the surface
dat.2nd <- SpatialGridDataFrame(grd, data.frame(var1.pred = predict(lm.2, newdata=grd))) 

# Clip the interpolated raster
r   <- raster(dat.2nd)

# Plot the map
tm_shape(r) + 
  tm_raster(n=10, palette="YlOrRd", 
            title="Predicted Ozone \n(in ppm)") +
  tm_shape(spSample) + tm_dots(size=0.2) +
  tm_legend(legend.position= c("left", "bottom")) +
  tm_layout("Second Order Polynomial Surface", inner.margins = 0.01) +
  tm_compass(type= "8star", position = c("right", "bottom"))


##Spatial Interpolation with Kriging (universal kriging)
#first order kriging

f.1 <- as.formula(PM25AGG ~ X + Y) 
f.2 <- as.formula(PM25AGG ~ X + Y + I(X*X)+I(Y*Y) + I(X*Y))

var.smpl <- variogram(f.1, spSample, cloud = FALSE) #, cutoff=1000000, width=89900)
dat.fit  <- fit.variogram(var.smpl, fit.ranges = FALSE, fit.sills = FALSE,
                          vgm(model="Exp", psill=4.5, range= 24))
plot(var.smpl, dat.fit)


# Define the trend model
f.1 <- as.formula(PM25AGG ~ X + Y) 

# Perform the krige interpolation (note the use of the variogram model
# created in the earlier step)
dat.krg <- krige( f.1, spSample, grd, dat.fit)

# Convert kriged surface to a raster object for clipping
r <- raster(dat.krg)

# Plot the map
tm_shape(r) + 
  tm_raster(n=10, palette="YlOrRd",  
            title="Predicted Ozone \n(in ppm)") +
  tm_shape(spSample) + tm_dots(size=0.2) +
  tm_legend(legend.position= c("left", "bottom")) +
  tm_layout("Universal Kriging Interpolation Surface", inner.margins = 0.01) +
  tm_compass(type= "8star", position = c("right", "bottom"))

r   <- raster(dat.krg, layer="var1.var")

tm_shape(r) + 
  tm_raster(n=7, palette ="YlOrRd",
            title="Variance map \n(in squared ppm)") +
  tm_shape(spSample) + tm_dots(size=0.2) +
  tm_legend(legend.position= c("left", "bottom")) +
  tm_layout("Variance Surface from Universal Kriging", inner.margins = 0.01) +
  tm_compass(type= "8star", position = c("right", "bottom"))

r   <- sqrt(raster(dat.krg, layer="var1.var")) * 1.96

tm_shape(r) + 
  tm_raster(n=7, palette ="YlOrRd",
            title="95% CI map \n(in ppm)") +
  tm_shape(spSample) + 
  tm_dots(size=0.2) +
  tm_legend(legend.position= c("left", "bottom")) +
  tm_layout("95% Confidence Interval for Universal Kriging Surface", inner.margins = 0.01) +
  tm_compass(type= "8star", position = c("right", "bottom"))

#2ndorder kriging

f.1 <- as.formula(PM25AGG ~ X + Y) 
f.2 <- as.formula(PM25AGG ~ X + Y + I(X*X)+I(Y*Y) + I(X*Y))

var.smpl <- variogram(f.2, spSample, cloud = FALSE) #, cutoff=1000000, width=89900)
dat.fit  <- fit.variogram(var.smpl, fit.ranges = FALSE, fit.sills = FALSE,
                          vgm(model="Sph", psill=0.87, range= 8))
plot(var.smpl, dat.fit)

# Define the trend model
f.1 <- as.formula(PM25AGG ~ X + Y) 

# Perform the krige interpolation (note the use of the variogram model
# created in the earlier step)
dat.krg <- krige( f.2, spSample, grd, dat.fit)

# Convert kriged surface to a raster object for clipping
r.f <- raster(dat.krg)

# Plot the map
tm_shape(r.f) + 
  tm_raster(n=10, palette="YlOrRd",  
            title="Predicted Ozone \n(in ppm)") +
  tm_shape(spSample) + tm_dots(size=0.2) +
  tm_legend(legend.position= c("left", "bottom")) +
  tm_layout("Universal Kriging Interpolation Surface", inner.margins = 0.01) +
  tm_compass(type= "8star", position = c("right", "bottom"))

r   <- raster(dat.krg, layer="var1.var")

tm_shape(r) + 
  tm_raster(n=7, palette ="YlOrRd",
            title="Variance map \n(in squared ppm)") +
  tm_shape(spSample) + tm_dots(size=0.2) +
  tm_legend(legend.position= c("left", "bottom")) +
  tm_layout("Variance Surface from Universal Kriging", inner.margins = 0.01) +
  tm_compass(type= "8star", position = c("right", "bottom"))

r   <- sqrt(raster(dat.krg, layer="var1.var")) * 1.96

tm_shape(r) + 
  tm_raster(n=7, palette ="YlOrRd",
            title="95% CI map \n(in ppm)") +
  tm_shape(spSample) + 
  tm_dots(size=0.2) +
  tm_legend(legend.position= c("left", "bottom")) +
  tm_layout("95% Confidence Interval for Universal Kriging Surface", inner.margins = 0.01) +
  tm_compass(type= "8star", position = c("right", "bottom"))


#########################################################
#These steps will help you combine the outputs from your spatial interpolation with your income data.

#If you have too many cells, you can reduce the number by aggregating values
step.1 <- aggregate(r.f, fact=1, fun=mean) ##r is raster from kriging
plot(step.1)

#Convert the raster dataset to points
step.2 <-  rasterToPoints(step.1,fun=NULL, spatial=FALSE, crs=AssignProjectionFromAnotherDataset)
step.2 <- as.data.frame(step.2) #convert the point dataset to a spatial dataframe
Coords <- step.2[,c("x", "y")]  #assign coordinates to a new object
crs <- crs(census.tracts) #utilize an existing projection
step.3 <- SpatialPointsDataFrame(coords = Coords, data = step.2, proj4string = crs) #create a spatial points dataframe
step.4 <- aggregate(x=step.3,by=income.tracts, FUN=mean) #aggregate points into census tracts
step.5 <- intersect(step.4,income.tracts)  #get the intersection of step.4 with the income.tracts dataset (this will take a while) 
step.5$var1.pred[step.5$var1.pred <=0 ] <- 0
#now ready to perform a regression

######Linear Regression##########
#Let's say your dataset with both PM2.5 and Income are stored in a dataset called income.pm25.ns.
income.pm25.ns <- step.5

#Plot income and PM2.5 from the income.pm25.ns dataset you created
plot(income.pm25.ns$Income~income.pm25.ns$var1.pred)
#Notice that there are a lot of 0's in this dataset. If you decide to remove them, use the following line:
income.pm25.ns <-  income.pm25.ns[!is.na(income.pm25.ns$var1.pred), ]

#Now plot the data again
plot(income.pm25.ns$Income~income.pm25.ns$var1.pred)

#Perform a linear regression on the two variables. You should decide which one is dependent.
lm.model <- lm(income.pm25.ns$Income~income.pm25.ns$var1.pred)
#Add the regression model to the plot you created
abline(lm.model)
#Get the summary of the results
summary(lm.model)

#You want to determine if the model residuals are spatially clustered. 
#First obtain the residuals from the model
model.resids <- as.data.frame(residuals.lm(lm.model))
#Then add the residuals to your spatialpolygon dataframe
income.pm25.ns$residuals <- residuals.lm(lm.model)
#Observe the result to make sure it looks correct
head(income.pm25.ns)

#Now, create choropleth map of residuals
resids <- income.pm25.ns$residuals
shades <- auto.shading(resids, n=6, cols = brewer.pal(6, 'Greens'))
choropleth(income.tracts, resids, shades, main = "Residuals from the Regression Analysis") #map the data with associated colours
choro.legend(-123.47, 49.13, shades, cex = 0.6, title = "Residuals") #add a legend (you might need to change the location)


#GLOBAL AND LOCAL MORANS I for Residuals from Regression

inc.nb <- poly2nb(income.pm25.ns)
inc.net <- nb2lines(inc.nb,coords=coordinates(income.pm25.ns)) #convert neighbour matrix into lines to plot - show for every polygon which are considered neighbours this is queens case 


tm_shape(income.pm25.ns) + tm_borders(col='lightgrey') + 
  tm_shape(inc.net) + tm_lines(col='red') +
  tm_add_legend(title = 'Legend', type= 'line', labels='Polygon Neighbours', col= 'red') +
  tm_layout(title= "Polygon Neighbours with Queens Case")

inc.nb2 <- poly2nb(income.tracts, queen = FALSE) # queen false therefore uses rooks case for second plot
inc.net2 <- nb2lines(inc.nb2,coords=coordinates(income.tracts))


tm_shape(income.pm25) + tm_borders(col='lightgrey') + 
  tm_shape(inc.net) + tm_lines(col='blue', lwd = 2) +
  tm_shape(inc.net2) + tm_lines(col='orange', lwd = 2) +
  tm_add_legend(title = 'Legend',type = 'line',labels = "Queens Case", col= 'blue', lwd=2) +
  tm_add_legend(type = 'line', labels = "Rooks Case", col='orange', lwd=2) +
  tm_layout(title= "Polygon Neighbours with Queens Case and Rooks Case")

######################## 

inc.lw <- nb2listw(inc.nb, zero.policy = TRUE, style = "W")
print.listw(inc.lw, zero.policy = TRUE) #weight matrix

###Global morans I on residuals
mi <- moran.test(resids, inc.lw, zero.policy = TRUE)
mi


moran.range <- function(lw) {
  wmat <- listw2mat(lw)
  return(range(eigen((wmat + t(wmat))/2)$values))
}
moran.range(inc.lw)


mI <- mi$estimate[[1]]
eI <- mi$estimate[[2]]
var <- mi$estimate[[3]]
z <-(mI-eI)/(sqrt(var))

mI <- round(mI, digits = 5)
eI<- round(eI, digits = 5)
var<- round(var, digits = 5)
z <- round(z, digits = 5)

######### make table

data.for.table2 = data.frame(mI, eI, var, z)

#Make table 2
table2 <- tableGrob(data.for.table2, cols=c("Moran's I", "Expected Moran's I", "Variance", "Z-Value"), rows = " ") #make a table "Graphical Object" (GrOb) 
t2Caption <- textGrob("Table 2: Global Moran's I for Residuals", gp = gpar(fontsize = 09))
padding <- unit(5, "mm")

table2 <- gtable_add_rows(table2, 
                          heights = grobHeight(t2Caption) + padding, 
                          pos = 0)

table2 <- gtable_add_grob(table2,
                          t2Caption, t = 1, l = 2, r = ncol(data.for.table2) + 1)


grid.arrange(table2, newpage = TRUE)

##Local morans I for residuals  
lisa.test <- localmoran(resids, inc.lw)
income.pm25.ns$Ii <- lisa.test[,1]
income.pm25.ns$E.Ii<- lisa.test[,2]
income.pm25.ns$Var.Ii<- lisa.test[,3]
income.pm25.ns$Z.Ii<- lisa.test[,4]
income.pm25.ns$P<- lisa.test[,5]

#map local morans I
map_LISA <- tm_shape(income.pm25.ns) + 
  tm_polygons(col = "Ii", 
              title = "Local Moran's I", 
              style = "fisher", 
              palette = "viridis", n = 6) 
map_LISA

###moran plot
moran.plot(resids, inc.lw, zero.policy=NULL, spChk=NULL, labels=NULL, xlab="Residuals", 
           ylab="Spatially Lagged Residuals", quiet=NULL)
           

####Geographically Weighted Regression

#continuing with data from the regression analysis. 

#The first thing you need to do is to add the polygon coordinates to the spatialpolygondataframe.
#You can obtain the coordinates using the "coordinates" function from the sp library
income.pm25.ns.coords <- sp::coordinates(income.pm25.ns)
#Observe the result
head(income.pm25.ns.coords)
#Now add the coordinates back to the spatialpolygondataframe
income.pm25.ns$X <- income.pm25.ns.coords[,1]
income.pm25.ns$Y <- income.pm25.ns.coords[,2]
head(income.pm25.ns)

###Determine the bandwidth for GWR: this will take a while
GWRbandwidth <- gwr.sel(income.pm25.ns$Income~income.pm25.ns$var1.pred, 
                        data=income.pm25, coords=cbind(income.pm25.ns$X,income.pm25.ns$Y),adapt=T) 

###Perform GWR on the two variables with the bandwidth determined above
###This will take a looooooong while
gwr.model = gwr(income.pm25.ns$Income~income.pm25.ns$var1.pred, 
                data=income.pm25.ns, coords=cbind(income.pm25.ns$X,income.pm25.ns$Y), 
                adapt=GWRbandwidth, hatmatrix=TRUE, se.fit=TRUE) 

#Print the results of the model
gwr.model

#Look at the results in detail
results<-as.data.frame(gwr.model$SDF)
head(results)

#Now for the magic. Let's add our local r-square values to the map
income.pm25.ns$localr <- results$localR2

#Create choropleth map of r-square values
local.r.square <- income.pm25.ns$localr
shades <- auto.shading(local.r.square, n=6, cols = brewer.pal(6, 'Oranges'))
choropleth(income.tracts, local.r.square, shades, main = "Local R2 Values from GWR") #map the data with associated colours
choro.legend(-123.47, 49.13, shades, cex = 0.6, title = "Local R2") #add a legend (you might need to change the location)

#Time for more magic. Let's map the coefficients
income.pm25.ns$coeff <- results$income.pm25.ns.var1.pred

#Create choropleth map of the coefficients
local.coefficient <- income.pm25.ns$coeff
shades <- auto.shading(local.coefficient, n=6, cols = brewer.pal(6, 'Oranges'))
choropleth(income.tracts, local.coefficient, shades, main = "Coefficients from GWR") #map the data with associated colours
choro.legend(-123.47, 49.13, shades, cex = 0.6, title = "Coefficients") #add a legend (you might need to change the location)

tmap_mode("view")

tm_shape(income.tracts) +
  tm_polygons( col= "white", lwd = 0.5) +
  tm_shape(local.coefficient) +
  tm_polygons("coeff") 


##POINT PATTERN ANALYSIS

points = spTransform(spSample, CRS("+init=epsg:26911"))

kma <- points

kma$x <- coordinates(kma)[,1]
kma$y <- coordinates(kma)[,2]
#check for and remove duplicated points
#check for duplicated points
#finds zero distance among points
zd <- zerodist(kma)
zd
#remove duplicates
kma <- remove.duplicates(kma)

#create an "extent" object which can be used to create the observation window for spatstat
kma.ext <- as.matrix(extent(kma)) 

#observation window
window <- as.owin(list(xrange = kma.ext[1,], yrange = kma.ext[2,]))

kma.ppp <- ppp(x = kma$x, y = kma$y, window = window)

quads <- 15

qcount <- quadratcount(kma.ppp, nx = quads, ny = quads)

plot(kma.ppp, pch = "+", cex = 0.5)
plot(qcount, add = T, col = "red")

qcount.df <- as.data.frame(qcount)

##Second, count the number of quadrats with a distinct number of points.

qcount.df <- plyr::count(qcount.df,'Freq')

##Change the column names so that x=number of points and f=frequency of quadrats with x point.

colnames(qcount.df) <- c("x","f")

sum.f.x2 <- sum(qcount.df$f*(qcount.df$x**2))

M <- 100

N <- sum((qcount.df$f)*(qcount.df$x))

sum.fx.2 <- sum(((qcount.df$f)*(qcount.df$x))**2)

VAR <- ((sum.f.x2 - ((sum.fx.2)/M))/(M-1))

MEAN <- N/M

VMR <- VAR/MEAN


##Finally, perform the test statistic to test for the existence of a random spatial pattern.

chi.square = VMR*(M-1)

p = 1 - pchisq(chi.square, (M - 1))



##K-FUNCTION 

#basic k-function

k.fun <- Kest(kma.ppp, correction = "Ripley")

plot(k.fun, main="K Function")


#use simulation to test the point pattern against CSR

k.fun.e <- envelope(kma.ppp, Kest, nsim = 99, correction = "Ripley")

plot(k.fun.e, main="K Function vs Complete Spatial Randomness")



###KERNEL DENSITY ESTIMATION

#2D (gaussian) kernel, compare how bandwidth (sigma) selection influences the point density estimates

#since data are projected, sigma is represented in metres

#eps is the width and height of the pixels (1000m X 1000m)

#coerce to a SpatialGridDataFrame for plotting

kde.100 <- density(kma.ppp, sigma = 100, at = "pixels", eps = c(100, 100))

kde.SG <- as(kde.100, "SpatialGridDataFrame")

kde.500 <- density(kma.ppp, sigma = 500, at = "pixels", eps = c(100, 100))

kde.SG <- cbind(kde.SG, as(kde.500, "SpatialGridDataFrame"))

kde.800 <- density(kma.ppp, sigma = 800, at = "pixels", eps = c(100, 100))

kde.SG <- cbind(kde.SG, as(kde.800, "SpatialGridDataFrame"))

kde.1000 <- density(kma.ppp, sigma = 1000, at = "pixels", eps = c(100, 100))

kde.SG <- cbind(kde.SG, as(kde.1000, "SpatialGridDataFrame"))


names(kde.SG) <- c("Bandwidth_100", "Bandwidth_500", "Bandwidth_800", "Bandwidth_1000")

#plot

spplot(kde.SG, main="Kernel Density Estimation with Varying Bandwidths")

#can see how the bandwidth selection influences the density estimates
summary(kde.SG)


#use cross-validation to get the bandwidth that minimizes MSE

bw.d <- bw.diggle(kma.ppp)

#plot the "optimal" bandwidth

plot(bw.d, ylim=c(-10, 10), main= "Optimal Bandwidth for the Kernel Density Estimation")

#density using the cross-validation bandwidth

kde.bwo <- density(kma.ppp, sigma = bw.d, at = "pixels", eps = c(100, 100))

plot(kde.bwo, main = "Kernel Density Estimation with Optimal Bandwidth")


##Study area map

tm_shape(income.tracts) +
  tm_polygons( col= "white", lwd = 0.5) +
  tm_shape(income.tracts) +
  tm_polygons( col = "yellowgreen", lwd=0.5) +
  tm_layout(title="Study Area: Greater Vancouver Metropolitan Area, BC", inner.margins = 0.1) +
  tm_compass(type= "8star", position = c("left", "bottom")) +
  tm_scale_bar()
